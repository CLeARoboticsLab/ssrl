run_name:
algo: ssrl
gpus: '0'
seed: 0
env: ant2
ssrl_forces_in_q_coords: True
ssrl_dynamics_fn: contact_integrate_only
ssrl_model_probabilistic: True

env_ant2:
  backend: generalized
  terminate_when_unhealthy: True

env_hopper2:
  backend: generalized
  terminate_when_unhealthy: True

env_walker2d2:
  backend: generalized
  terminate_when_unhealthy: True

ssrl:
  episode_length: 1000
  policy_repeat: 1
  num_epochs: 80  # N
  model_trains_per_epoch: 1
  training_steps_per_model_train: 1
  env_steps_per_training_step: 1000
  model_rollouts_per_hallucination_update: 400
  sac_grad_updates_per_hallucination_update: 20
  init_exploration_steps: 1000
  clear_model_buffer_after_model_train: True
  action_repeat: 1
  obs_history_length: 1
  num_envs: 1
  num_evals: 81
  num_eval_envs: 1
  policy_normalize_observations: False
  model_learning_rate: 1e-3
  model_training_batch_size: 250
  model_training_max_epochs: 2000
  model_training_convergence_criteria: 0.01
  model_training_consec_converged_epochs: 6
  model_training_abs_criteria: null
  model_training_test_ratio: 0.2
  model_training_weight_decay: True
  model_training_stop_gradient: False
  model_loss_horizon: 1
  model_check_done_condition: True
  max_env_buffer_size: null
  max_model_buffer_size: null
  sac_learning_rate: 0.0003
  sac_discounting: 0.99
  sac_batch_size: 256
  real_ratio: 0.06
  sac_reward_scaling: 1.0
  sac_tau: 0.005
  sac_fixed_alpha: 0.2
  seed: ${seed}
  deterministic_in_env: False
  deterministic_eval: False
  hallucination_max_std: -1.0
  zero_final_layer_of_policy: False

ssrl_linear_threshold_fn:  # k
  start_epoch: 20
  end_epoch: 150
  start_model_horizon: 1
  end_model_horizon: 15

ssrl_hupts_fn:
  start_epoch: 0
  end_epoch: 9
  start_hupts: 100
  end_hupts: 1000

wandb:
  entity: your-org-here
  log: False